{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "# Transform Raw Data to Sample Data",
   "id": "a180f7fd638534b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T13:40:42.240405Z",
     "start_time": "2026-02-08T13:40:41.992588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!uv pip install \\\n",
    "  \"fastparquet>=2025.12.0\" \\\n",
    "  \"ipykernel>=7.2.0\" \\\n",
    "  \"llvmlite>=0.42\" \\\n",
    "  \"matplotlib>=3.10.8\" \\\n",
    "  \"numba>=0.59\" \\\n",
    "  \"numpy>=2.3.5\" \\\n",
    "  \"pandas==2.3.3\" \\\n",
    "  \"pandas-stubs==2.3.3.260113\" \\\n",
    "  \"prophet>=1.3.0\" \\\n",
    "  \"scikit-learn>=1.8.0\" \\\n",
    "  \"scikit-learn>=1.8.0\" \\\n",
    "  \"scipy>=1.17.0\" \\\n",
    "  \"seaborn>=0.13.2\" \\\n",
    "  \"umap-learn>=0.5.11\""
   ],
   "id": "4d40e02b01deb921",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2mUsing Python 3.12.12 environment at: /Users/z.yang/playground/srh-stat-and-ml-exam/.venv\u001B[0m\r\n",
      "\u001B[2mAudited \u001B[1m14 packages\u001B[0m \u001B[2min 32ms\u001B[0m\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Because supervised and unsupervised learning require different data formats, the same dataset is transformed into two separate sample datasets according to the problem definition.",
   "id": "b7a0cf807699571c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T13:42:18.542256Z",
     "start_time": "2026-02-08T13:42:18.531577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT_DIR = Path().resolve().parent\n",
    "DATA_DIR = ROOT_DIR / \"data\""
   ],
   "id": "8b24eb0ef510649a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T13:40:44.993922Z",
     "start_time": "2026-02-08T13:40:44.970402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"future.no_silent_downcasting\", True)"
   ],
   "id": "bc39f7e0a301f6a5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Format Check",
   "id": "9e90b7a1e3439f1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T13:40:46.864355Z",
     "start_time": "2026-02-08T13:40:46.776138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "DATE_RE = re.compile(r\"^\\d{2}-\\d{2}-\\d{4}$\")                 # MM-DD-YYYY\n",
    "TIME_RE = re.compile(r\"^\\d{1,2}:\\d{2}$\")                     # H:MM or HH:MM\n",
    "INT_RE  = re.compile(r\"^-?\\d+$\")                             # integer\n",
    "NUM_RE  = re.compile(r\"^-?\\d+(\\.\\d+)?$\")                     # number (int/float)\n",
    "\n",
    "\n",
    "def is_valid_record_line(line: str) -> bool:\n",
    "    \"\"\"\n",
    "    Accepts a raw line. Returns True if it matches the expected 4-field format:\n",
    "    date<TAB>time<TAB>code<TAB>value\n",
    "    \"\"\"\n",
    "    line = line.rstrip(\"\\n\")\n",
    "    if not line.strip():\n",
    "        return True  # ignore empty/whitespace-only lines\n",
    "\n",
    "    parts = line.split(\"\\t\")\n",
    "    if len(parts) != 4:\n",
    "        return False\n",
    "\n",
    "    date_s, time_s, code_s, value_s = [p.strip() for p in parts]\n",
    "\n",
    "    # must be non-empty\n",
    "    if not date_s or not time_s or not code_s or not value_s:\n",
    "        return False\n",
    "\n",
    "    if not DATE_RE.match(date_s):\n",
    "        return False\n",
    "    if not TIME_RE.match(time_s):\n",
    "        return False\n",
    "    if not INT_RE.match(code_s):\n",
    "        return False\n",
    "    if not NUM_RE.match(value_s):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def scan_file(fp: Path, max_examples: int = 5) -> Tuple[bool, List[Tuple[int, str]]]:\n",
    "    \"\"\"\n",
    "    Returns (is_bad, examples). examples are (line_no, raw_line) for invalid lines.\n",
    "    \"\"\"\n",
    "    examples: List[Tuple[int, str]] = []\n",
    "    with fp.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        for i, line in enumerate(f, start=1):\n",
    "            if not is_valid_record_line(line):\n",
    "                if len(examples) < max_examples:\n",
    "                    examples.append((i, line.rstrip(\"\\n\")))\n",
    "                # keep scanning to decide bad file; but examples limited\n",
    "    return (len(examples) > 0), examples\n",
    "\n",
    "\n",
    "def find_bad_files(data_dir: Path, pattern: str = \"data-*\", max_examples: int = 5) -> None:\n",
    "    bad_files: List[Path] = []\n",
    "    details = {}\n",
    "\n",
    "    for fp in sorted(data_dir.glob(pattern)):\n",
    "        if fp.is_dir():\n",
    "            continue\n",
    "        is_bad, examples = scan_file(fp, max_examples=max_examples)\n",
    "        if is_bad:\n",
    "            bad_files.append(fp)\n",
    "            details[fp.name] = examples\n",
    "\n",
    "    # 1) output bad file list\n",
    "    out_list = data_dir / \"bad_format_files.txt\"\n",
    "    with out_list.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for fp in bad_files:\n",
    "            f.write(fp.name + \"\\n\")\n",
    "\n",
    "    # 2) output examples per bad file\n",
    "    out_examples = data_dir / \"bad_format_examples.txt\"\n",
    "    with out_examples.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for name in sorted(details.keys()):\n",
    "            f.write(f\"== {name} ==\\n\")\n",
    "            for line_no, raw in details[name]:\n",
    "                f.write(f\"  line {line_no}: {raw}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    print(f\"Scanned dir: {data_dir.resolve()}\")\n",
    "    print(f\"Bad files: {len(bad_files)}\")\n",
    "    print(f\"Bad file names: {[fp.name for fp in bad_files]}\")\n",
    "    print(f\"Written: {out_list}  (file names only)\")\n",
    "    print(f\"Written: {out_examples}  (first {max_examples} invalid lines per file)\")\n",
    "\n",
    "\n",
    "# change this to your folder that contains data-01..data-70\n",
    "find_bad_files(DATA_DIR / \"raw\", pattern=\"data-*\", max_examples=5)"
   ],
   "id": "82e009ba7fe53b07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanned dir: /Users/z.yang/playground/srh-stat-and-ml-exam/data/raw\n",
      "Bad files: 5\n",
      "Bad file names: ['data-02', 'data-27', 'data-29', 'data-40', 'data-67']\n",
      "Written: /Users/z.yang/playground/srh-stat-and-ml-exam/data/raw/bad_format_files.txt  (file names only)\n",
      "Written: /Users/z.yang/playground/srh-stat-and-ml-exam/data/raw/bad_format_examples.txt  (first 5 invalid lines per file)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Unsupervised Learning",
   "id": "f76502c6f166917e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T13:43:49.479750Z",
     "start_time": "2026-02-08T13:43:46.383603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "BG_CODES = [48, 57, 58, 59, 60, 61, 62, 63, 64]\n",
    "INS_CODES = [33, 34, 35]\n",
    "EVT_CODES = [65, 66, 67, 68, 69, 70, 71, 72]\n",
    "PAPER_TIMES = {\"08:00\", \"12:00\", \"18:00\", \"22:00\"}\n",
    "\n",
    "\n",
    "def _to_minutes(hhmm: str) -> int:\n",
    "    h, m = hhmm.split(\":\")\n",
    "    return int(h) * 60 + int(m)\n",
    "\n",
    "\n",
    "def _quantile(x: pd.Series, q: float) -> float:\n",
    "    # robust quantile helper (returns nan if empty)\n",
    "    if x.empty:\n",
    "        return np.nan\n",
    "    return float(x.quantile(q))\n",
    "\n",
    "\n",
    "def _stats_for_series(x: pd.Series) -> dict:\n",
    "    # x is numeric series already\n",
    "    if x.empty:\n",
    "        return {\n",
    "            \"count\": 0,\n",
    "            \"min\": np.nan,\n",
    "            \"max\": np.nan,\n",
    "            \"mean\": np.nan,\n",
    "            \"median\": np.nan,\n",
    "            \"std\": np.nan,\n",
    "            \"first\": np.nan,\n",
    "            \"last\": np.nan,\n",
    "            \"range\": np.nan,\n",
    "        }\n",
    "    x_sorted = x  # assume caller has sorted by time if needed\n",
    "    mn = float(x_sorted.min())\n",
    "    mx = float(x_sorted.max())\n",
    "    return {\n",
    "        \"count\": int(x_sorted.shape[0]),\n",
    "        \"min\": mn,\n",
    "        \"max\": mx,\n",
    "        \"mean\": float(x_sorted.mean()),\n",
    "        \"median\": float(x_sorted.median()),\n",
    "        \"std\": float(x_sorted.std(ddof=1)) if x_sorted.shape[0] > 1 else 0.0,\n",
    "        \"first\": float(x_sorted.iloc[0]),\n",
    "        \"last\": float(x_sorted.iloc[-1]),\n",
    "        \"range\": float(mx - mn),\n",
    "    }\n",
    "\n",
    "\n",
    "def read_diabetes_files(\n",
    "        data_dir: Path,\n",
    "        pattern: str = \"data-*\",\n",
    "        bad_files: List[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads all diabetes files into one long table:\n",
    "    columns: id, date, time, code, value\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    bad_files = bad_files or []\n",
    "    for fp in sorted(data_dir.glob(pattern)):\n",
    "        if fp.is_dir():\n",
    "            continue\n",
    "        file_id = fp.name  # expects \"data-01\", ...\n",
    "        if file_id in bad_files:  # Skip files with bad format\n",
    "            continue\n",
    "        df = pd.read_csv(\n",
    "            fp,\n",
    "            sep=\"\\t\",\n",
    "            header=None,\n",
    "            names=[\"date_str\", \"time_str\", \"code\", \"value_str\"],\n",
    "            dtype={\"date_str\": \"string\", \"time_str\": \"string\", \"code\": \"int64\", \"value_str\": \"string\"},\n",
    "        )\n",
    "        df[\"id\"] = file_id\n",
    "        rows.append(df)\n",
    "\n",
    "    if not rows:\n",
    "        raise FileNotFoundError(f\"No files matched {pattern} under {data_dir}\")\n",
    "\n",
    "    out = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "    # parse date/time\n",
    "    out[\"date\"] = pd.to_datetime(out[\"date_str\"], format=\"%m-%d-%Y\", errors=\"coerce\")\n",
    "    out[\"time\"] = out[\"time_str\"].astype(str).str.zfill(5)  # e.g. \"9:09\" -> \"09:09\" if needed\n",
    "    out[\"time_minutes\"] = out[\"time\"].map(_to_minutes)\n",
    "\n",
    "    # parse value: keep numeric, tolerate leading zeros like \"009\"\n",
    "    out[\"value\"] = pd.to_numeric(out[\"value_str\"], errors=\"coerce\")\n",
    "\n",
    "    # basic validation\n",
    "    bad = out[\"date\"].isna() | out[\"time_minutes\"].isna() | out[\"value\"].isna()\n",
    "    if bad.any():\n",
    "        # keep them out, but you can also raise\n",
    "        out = out.loc[~bad].copy()\n",
    "\n",
    "    return out[[\"id\", \"date\", \"time\", \"time_minutes\", \"code\", \"value\"]]\n",
    "\n",
    "\n",
    "def build_daily_features(long_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Input: long_df with columns: id, date, time, time_minutes, code, value\n",
    "    Output: wide daily feature table\n",
    "    \"\"\"\n",
    "    # sort for first/last features\n",
    "    long_df = long_df.sort_values([\"id\", \"date\", \"time_minutes\", \"code\"], ascending=True).copy()\n",
    "\n",
    "    # base daily aggregates (time rhythm + counts)\n",
    "    g = long_df.groupby([\"id\", \"date\"], sort=False)\n",
    "\n",
    "    base = g.agg(\n",
    "        n_events=(\"code\", \"size\"),\n",
    "        n_unique_timestamps=(\"time\", \"nunique\"),\n",
    "        first_time_minutes=(\"time_minutes\", \"min\"),\n",
    "        last_time_minutes=(\"time_minutes\", \"max\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    base[\"active_span_minutes\"] = base[\"last_time_minutes\"] - base[\"first_time_minutes\"]\n",
    "\n",
    "    # time buckets (record counts)\n",
    "    def _bucket_counts(sub: pd.DataFrame) -> pd.Series:\n",
    "        t = sub[\"time_minutes\"].to_numpy()\n",
    "        # [0, 300) night (00:00-04:59)\n",
    "        night = int(((t >= 0) & (t < 300)).sum())\n",
    "        morning = int(((t >= 300) & (t < 720)).sum())      # 05:00-11:59\n",
    "        afternoon = int(((t >= 720) & (t < 1080)).sum())   # 12:00-17:59\n",
    "        evening = int(((t >= 1080) & (t < 1440)).sum())    # 18:00-23:59\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"events_night_count\": night,\n",
    "                \"events_morning_count\": morning,\n",
    "                \"events_afternoon_count\": afternoon,\n",
    "                \"events_evening_count\": evening,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    buckets = g.apply(_bucket_counts, include_groups=False).reset_index()\n",
    "\n",
    "    # paper-like time flag: share of events at fixed logical times\n",
    "    def _paper_like_flag(sub: pd.DataFrame) -> bool:\n",
    "        # normalize \"09:09\" etc already\n",
    "        share = (sub[\"time\"].isin(PAPER_TIMES)).mean()\n",
    "        return bool(share >= 0.6)  # threshold; adjust as needed\n",
    "\n",
    "    paper_flag = g.apply(\n",
    "        _paper_like_flag,\n",
    "        include_groups=False).reset_index(name=\"has_paper_like_times_flag\")\n",
    "\n",
    "    # add calendar features\n",
    "    base[\"dow\"] = pd.to_datetime(base[\"date\"]).dt.weekday.astype(int)\n",
    "    base[\"is_weekend\"] = base[\"dow\"].isin([5, 6])\n",
    "\n",
    "    # per-code stats builder\n",
    "    def build_code_stats(codes: Iterable[int], prefix: str) -> pd.DataFrame:\n",
    "        pieces = []\n",
    "        for c in codes:\n",
    "            sub = long_df.loc[long_df[\"code\"] == c, [\"id\", \"date\", \"time_minutes\", \"value\"]].copy()\n",
    "            if sub.empty:\n",
    "                # create empty frame with expected columns later via merge\n",
    "                continue\n",
    "            sub = sub.sort_values([\"id\", \"date\", \"time_minutes\"])\n",
    "            gs = sub.groupby([\"id\", \"date\"], sort=False)[\"value\"].apply(\n",
    "                lambda s: pd.Series(_stats_for_series(s)))\n",
    "            gs = gs.unstack()  # columns are the stats keys\n",
    "            gs = gs.add_prefix(f\"{prefix}_{c}_\").reset_index()\n",
    "            # count should be int, keep others float\n",
    "            if f\"{prefix}_{c}_count\" in gs.columns:\n",
    "                gs[f\"{prefix}_{c}_count\"] = gs[f\"{prefix}_{c}_count\"].fillna(0).astype(int)\n",
    "            pieces.append(gs)\n",
    "\n",
    "        if not pieces:\n",
    "            return base[[\"id\", \"date\"]].copy()\n",
    "        out = pieces[0]\n",
    "        for p in pieces[1:]:\n",
    "            out = out.merge(p, on=[\"id\", \"date\"], how=\"outer\")\n",
    "        return out\n",
    "\n",
    "    bg_stats = build_code_stats(BG_CODES, \"bg\")\n",
    "    ins_stats = build_code_stats(INS_CODES, \"ins\")\n",
    "\n",
    "    # merged BG \"all\" stats\n",
    "    bg_all = long_df.loc[long_df[\"code\"].isin(BG_CODES), [\"id\", \"date\", \"time_minutes\", \"value\"]].copy()\n",
    "    if not bg_all.empty:\n",
    "        bg_all = bg_all.sort_values([\"id\", \"date\", \"time_minutes\"])\n",
    "        bg_all_g = bg_all.groupby([\"id\", \"date\"], sort=False)[\"value\"].apply(\n",
    "            lambda s: pd.Series(_stats_for_series(s)))\n",
    "        bg_all_g = bg_all_g.unstack().add_prefix(\"bg_all_\").reset_index()\n",
    "        bg_all_g[\"bg_all_count\"] = bg_all_g[\"bg_all_count\"].fillna(0).astype(int)\n",
    "    else:\n",
    "        bg_all_g = base[[\"id\", \"date\"]].copy()\n",
    "\n",
    "    # merged INS \"all\" stats\n",
    "    ins_all = long_df.loc[long_df[\"code\"].isin(INS_CODES), [\"id\", \"date\", \"time_minutes\", \"value\"]].copy()\n",
    "    if not ins_all.empty:\n",
    "        ins_all = ins_all.sort_values([\"id\", \"date\", \"time_minutes\"])\n",
    "        ins_all_g = ins_all.groupby([\"id\", \"date\"], sort=False)[\"value\"].apply(\n",
    "            lambda s: pd.Series(_stats_for_series(s)))\n",
    "        ins_all_g = ins_all_g.unstack().add_prefix(\"ins_all_\").reset_index()\n",
    "        ins_all_g[\"ins_all_count\"] = ins_all_g[\"ins_all_count\"].fillna(0).astype(int)\n",
    "        # \"sum\" for insulin all: add explicitly\n",
    "        ins_sum = ins_all.groupby([\"id\", \"date\"], sort=False)[\"value\"].sum().reset_index(name=\"ins_all_sum\")\n",
    "        ins_all_g = ins_all_g.merge(ins_sum, on=[\"id\", \"date\"], how=\"left\")\n",
    "    else:\n",
    "        ins_all_g = base[[\"id\", \"date\"]].copy()\n",
    "\n",
    "    # per-code insulin sum (dose sum is useful)\n",
    "    for c in INS_CODES:\n",
    "        col = f\"ins_{c}_sum\"\n",
    "        if col not in ins_stats.columns:\n",
    "            # compute sum per code\n",
    "            tmp = long_df.loc[long_df[\"code\"] == c].groupby([\"id\", \"date\"], sort=False)[\"value\"].sum().reset_index(name=col)\n",
    "            ins_stats = ins_stats.merge(tmp, on=[\"id\", \"date\"], how=\"left\")\n",
    "\n",
    "    # events: count + flag\n",
    "    evt = long_df.loc[long_df[\"code\"].isin(EVT_CODES), [\"id\", \"date\", \"code\"]].copy()\n",
    "    if evt.empty:\n",
    "        evt_wide = base[[\"id\", \"date\"]].copy()\n",
    "    else:\n",
    "        evt_counts = (\n",
    "            evt.groupby([\"id\", \"date\", \"code\"], sort=False)\n",
    "            .size()\n",
    "            .rename(\"count\")\n",
    "            .reset_index()\n",
    "        )\n",
    "        # pivot counts\n",
    "        evt_wide = evt_counts.pivot_table(index=[\"id\", \"date\"], columns=\"code\", values=\"count\", fill_value=0, aggfunc=\"sum\")\n",
    "        evt_wide.columns = [f\"evt_{int(c)}_count\" for c in evt_wide.columns]\n",
    "        evt_wide = evt_wide.reset_index()\n",
    "        # flags\n",
    "        for c in EVT_CODES:\n",
    "            ccol = f\"evt_{c}_count\"\n",
    "            fcol = f\"evt_{c}_flag\"\n",
    "            if ccol not in evt_wide.columns:\n",
    "                evt_wide[ccol] = 0\n",
    "            evt_wide[fcol] = evt_wide[ccol].astype(int) > 0\n",
    "\n",
    "    # rollups\n",
    "    def ensure_col(df: pd.DataFrame, col: str, default=0):\n",
    "        if col not in df.columns:\n",
    "            df[col] = default\n",
    "        return df\n",
    "\n",
    "    evt_wide = ensure_col(evt_wide, \"evt_66_count\", 0)\n",
    "    evt_wide = ensure_col(evt_wide, \"evt_67_count\", 0)\n",
    "    evt_wide = ensure_col(evt_wide, \"evt_68_count\", 0)\n",
    "    evt_wide = ensure_col(evt_wide, \"evt_69_count\", 0)\n",
    "    evt_wide = ensure_col(evt_wide, \"evt_70_count\", 0)\n",
    "    evt_wide = ensure_col(evt_wide, \"evt_71_count\", 0)\n",
    "    evt_wide = ensure_col(evt_wide, \"evt_65_flag\", False)\n",
    "    evt_wide = ensure_col(evt_wide, \"evt_72_flag\", False)\n",
    "\n",
    "    evt_wide[\"meal_events_count\"] = evt_wide[\"evt_66_count\"] + evt_wide[\"evt_67_count\"] + evt_wide[\"evt_68_count\"]\n",
    "    evt_wide[\"meal_more_flag\"] = ensure_col(evt_wide, \"evt_67_flag\", False)[\"evt_67_flag\"]\n",
    "    evt_wide[\"meal_less_flag\"] = ensure_col(evt_wide, \"evt_68_flag\", False)[\"evt_68_flag\"]\n",
    "\n",
    "    evt_wide[\"exercise_events_count\"] = evt_wide[\"evt_69_count\"] + evt_wide[\"evt_70_count\"] + evt_wide[\"evt_71_count\"]\n",
    "    evt_wide[\"exercise_more_flag\"] = ensure_col(evt_wide, \"evt_70_flag\", False)[\"evt_70_flag\"]\n",
    "    evt_wide[\"exercise_less_flag\"] = ensure_col(evt_wide, \"evt_71_flag\", False)[\"evt_71_flag\"]\n",
    "\n",
    "    evt_wide[\"hypo_flag\"] = ensure_col(evt_wide, \"evt_65_flag\", False)[\"evt_65_flag\"]\n",
    "    evt_wide[\"special_flag\"] = ensure_col(evt_wide, \"evt_72_flag\", False)[\"evt_72_flag\"]\n",
    "\n",
    "    # missing flags\n",
    "    bg_presence = long_df.groupby([\"id\", \"date\"], sort=False)[\"code\"].apply(lambda s: bool(s.isin(BG_CODES).any())).reset_index(name=\"has_bg\")\n",
    "    ins_presence = long_df.groupby([\"id\", \"date\"], sort=False)[\"code\"].apply(lambda s: bool(s.isin(INS_CODES).any())).reset_index(name=\"has_ins\")\n",
    "\n",
    "    # merge everything\n",
    "    daily = (\n",
    "        base.merge(buckets, on=[\"id\", \"date\"], how=\"left\")\n",
    "            .merge(paper_flag, on=[\"id\", \"date\"], how=\"left\")\n",
    "            .merge(bg_stats, on=[\"id\", \"date\"], how=\"left\")\n",
    "            .merge(bg_all_g, on=[\"id\", \"date\"], how=\"left\")\n",
    "            .merge(ins_stats, on=[\"id\", \"date\"], how=\"left\")\n",
    "            .merge(ins_all_g, on=[\"id\", \"date\"], how=\"left\")\n",
    "            .merge(evt_wide, on=[\"id\", \"date\"], how=\"left\")\n",
    "            .merge(bg_presence, on=[\"id\", \"date\"], how=\"left\")\n",
    "            .merge(ins_presence, on=[\"id\", \"date\"], how=\"left\")\n",
    "    )\n",
    "\n",
    "    # finalize flags\n",
    "    daily[\"has_paper_like_times_flag\"] = daily[\"has_paper_like_times_flag\"].fillna(False).astype(bool)\n",
    "    daily[\"missing_bg_flag\"] = (~daily[\"has_bg\"].fillna(False)).astype(bool)\n",
    "    daily[\"missing_insulin_flag\"] = (~daily[\"has_ins\"].fillna(False)).astype(bool)\n",
    "    daily = daily.drop(columns=[\"has_bg\", \"has_ins\"])\n",
    "\n",
    "    # fill count columns that may be missing due to merges\n",
    "    for c in BG_CODES:\n",
    "        cnt = f\"bg_{c}_count\"\n",
    "        if cnt in daily.columns:\n",
    "            daily[cnt] = daily[cnt].fillna(0).astype(int)\n",
    "    for c in INS_CODES:\n",
    "        cnt = f\"ins_{c}_count\"\n",
    "        if cnt in daily.columns:\n",
    "            daily[cnt] = daily[cnt].fillna(0).astype(int)\n",
    "    for c in EVT_CODES:\n",
    "        cnt = f\"evt_{c}_count\"\n",
    "        flg = f\"evt_{c}_flag\"\n",
    "        if cnt in daily.columns:\n",
    "            daily[cnt] = daily[cnt].fillna(0).astype(int)\n",
    "        if flg in daily.columns:\n",
    "            daily[flg] = daily[flg].astype(\"boolean\").fillna(False).astype(bool)\n",
    "\n",
    "    # ensure deterministic column order: keys first\n",
    "    key_cols = [\"id\", \"date\"]\n",
    "    other_cols = [c for c in daily.columns if c not in key_cols]\n",
    "    daily = daily[key_cols + other_cols].sort_values([\"id\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "    return daily\n",
    "\n",
    "\n",
    "bad_files = [\"data-02\", \"data-27\", \"data-29\", \"data-40\", \"data-67\"]\n",
    "long_df = read_diabetes_files(DATA_DIR / \"raw\", pattern=\"data-*\", bad_files=bad_files)\n",
    "daily_df = build_daily_features(long_df)\n",
    "\n",
    "OUT_DIR = ROOT_DIR / \"data\" / \"processed\" / \"unsupervised\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "daily_df.to_parquet(OUT_DIR / \"diabetes_daily_features.parquet\", engine=\"fastparquet\", index=False)\n",
    "print(daily_df.shape)\n",
    "print(daily_df.head())"
   ],
   "id": "f9aacc8cd49f9ea6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3378, 170)\n",
      "        id       date  n_events  n_unique_timestamps  first_time_minutes  \\\n",
      "0  data-01 1991-04-21         6                    3                 549   \n",
      "1  data-01 1991-04-22         6                    3                 455   \n",
      "2  data-01 1991-04-23         5                    2                 445   \n",
      "3  data-01 1991-04-24         7                    4                 472   \n",
      "4  data-01 1991-04-25         8                    4                 449   \n",
      "\n",
      "   last_time_minutes  active_span_minutes  dow  is_weekend  \\\n",
      "0               1371                  822    6        True   \n",
      "1               1016                  561    0       False   \n",
      "2               1045                  600    1       False   \n",
      "3               1329                  857    2       False   \n",
      "4               1314                  865    3       False   \n",
      "\n",
      "   events_night_count  ...  meal_events_count  meal_more_flag  meal_less_flag  \\\n",
      "0                   0  ...                NaN             NaN             NaN   \n",
      "1                   0  ...                NaN             NaN             NaN   \n",
      "2                   0  ...                NaN             NaN             NaN   \n",
      "3                   0  ...                NaN             NaN             NaN   \n",
      "4                   0  ...                NaN             NaN             NaN   \n",
      "\n",
      "   exercise_events_count  exercise_more_flag  exercise_less_flag  hypo_flag  \\\n",
      "0                    NaN                 NaN                 NaN        NaN   \n",
      "1                    NaN                 NaN                 NaN        NaN   \n",
      "2                    NaN                 NaN                 NaN        NaN   \n",
      "3                    NaN                 NaN                 NaN        NaN   \n",
      "4                    NaN                 NaN                 NaN        NaN   \n",
      "\n",
      "   special_flag  missing_bg_flag  missing_insulin_flag  \n",
      "0           NaN            False                 False  \n",
      "1           NaN            False                 False  \n",
      "2           NaN            False                 False  \n",
      "3           NaN            False                 False  \n",
      "4           NaN            False                 False  \n",
      "\n",
      "[5 rows x 170 columns]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T15:20:56.833404Z",
     "start_time": "2026-02-07T15:20:56.828515Z"
    }
   },
   "cell_type": "markdown",
   "source": "---",
   "id": "b09aa5a45e846f5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Supervised Learning",
   "id": "c62fc469559051af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T13:44:12.213046Z",
     "start_time": "2026-02-08T13:44:02.034618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from typing import Iterable, List, Dict, Optional\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Constants (UCI Diabetes codes)\n",
    "# -----------------------------\n",
    "BG_CODES = {48, 57, 58, 59, 60, 61, 62, 63, 64}\n",
    "INS_CODES = {33, 34, 35}\n",
    "MEAL_CODES = {66, 67, 68}\n",
    "EXERCISE_CODES = {69, 70, 71}\n",
    "HYPO_CODE = 65\n",
    "SPECIAL_CODE = 72\n",
    "\n",
    "PAPER_TIMES = {\"08:00\", \"12:00\", \"18:00\", \"22:00\"}  # paper logical slots\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def _normalize_time_str(x: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize HH:MM to zero-padded form.\n",
    "    Examples: '9:09' -> '09:09', '23:0' -> '23:00'\n",
    "    \"\"\"\n",
    "    s = str(x).strip()\n",
    "    if not s or s.lower() == \"nan\":\n",
    "        return \"\"\n",
    "    # split and pad\n",
    "    if \":\" not in s:\n",
    "        return \"\"\n",
    "    hh, mm = s.split(\":\", 1)\n",
    "    hh = hh.strip().zfill(2)\n",
    "    mm = mm.strip().zfill(2)\n",
    "    return f\"{hh}:{mm}\"\n",
    "\n",
    "\n",
    "def _to_minutes(hhmm: str) -> int:\n",
    "    hh, mm = hhmm.split(\":\")\n",
    "    return int(hh) * 60 + int(mm)\n",
    "\n",
    "\n",
    "def _safe_std(x: pd.Series) -> float:\n",
    "    if x.shape[0] <= 1:\n",
    "        return 0.0\n",
    "    return float(x.std(ddof=1))\n",
    "\n",
    "\n",
    "def _series_stats(x: pd.Series) -> Dict[str, float]:\n",
    "    if x.empty:\n",
    "        return {\n",
    "            \"count\": 0,\n",
    "            \"sum\": 0.0,\n",
    "            \"min\": np.nan,\n",
    "            \"max\": np.nan,\n",
    "            \"mean\": np.nan,\n",
    "            \"median\": np.nan,\n",
    "            \"std\": np.nan,\n",
    "            \"range\": np.nan,\n",
    "        }\n",
    "    mn = float(x.min())\n",
    "    mx = float(x.max())\n",
    "    return {\n",
    "        \"count\": int(x.shape[0]),\n",
    "        \"sum\": float(x.sum()),\n",
    "        \"min\": mn,\n",
    "        \"max\": mx,\n",
    "        \"mean\": float(x.mean()),\n",
    "        \"median\": float(x.median()),\n",
    "        \"std\": _safe_std(x),\n",
    "        \"range\": float(mx - mn),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Read raw event logs (long)\n",
    "# -----------------------------\n",
    "def read_diabetes_files_long(\n",
    "    data_dir: Path,\n",
    "    pattern: str = \"data-*\",\n",
    "    bad_files: List[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads raw UCI Diabetes files into one long event table.\n",
    "\n",
    "    Output schema:\n",
    "      id (string), datetime (datetime64[ns]), date (datetime64[ns]), time (HH:MM string),\n",
    "      time_minutes (int), code (int), value (float)\n",
    "    \"\"\"\n",
    "    bad_files = bad_files or []\n",
    "    rows = []\n",
    "\n",
    "    for fp in sorted(data_dir.glob(pattern)):\n",
    "        if fp.is_dir():\n",
    "            continue\n",
    "\n",
    "        file_id = fp.name  # expects \"data-01\", ...\n",
    "        if file_id in bad_files:\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(\n",
    "            fp,\n",
    "            sep=\"\\t\",\n",
    "            header=None,\n",
    "            names=[\"date_str\", \"time_str\", \"code\", \"value_str\"],\n",
    "            dtype={\"date_str\": \"string\", \"time_str\": \"string\", \"code\": \"string\", \"value_str\": \"string\"},\n",
    "        )\n",
    "\n",
    "        df[\"id\"] = file_id\n",
    "\n",
    "        # normalize & parse\n",
    "        df[\"time\"] = df[\"time_str\"].astype(str).map(_normalize_time_str)\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date_str\"], format=\"%m-%d-%Y\", errors=\"coerce\")\n",
    "        df[\"code\"] = pd.to_numeric(df[\"code\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        df[\"value\"] = pd.to_numeric(df[\"value_str\"], errors=\"coerce\")\n",
    "\n",
    "        # basic row validity checks\n",
    "        # - require parseable date and HH:MM\n",
    "        # - require numeric code\n",
    "        # - require numeric value for codes that should carry values (BG + INS)\n",
    "        has_valid_time = df[\"time\"].str.match(r\"^\\d{2}:\\d{2}$\", na=False)\n",
    "        df = df.loc[df[\"date\"].notna() & has_valid_time & df[\"code\"].notna()].copy()\n",
    "\n",
    "        # construct datetime and time_minutes\n",
    "        df[\"datetime\"] = pd.to_datetime(\n",
    "            df[\"date\"].dt.strftime(\"%Y-%m-%d\") + \" \" + df[\"time\"],\n",
    "            format=\"%Y-%m-%d %H:%M\",\n",
    "            errors=\"coerce\",\n",
    "        )\n",
    "        df = df.loc[df[\"datetime\"].notna()].copy()\n",
    "        df[\"time_minutes\"] = df[\"time\"].map(_to_minutes).astype(int)\n",
    "\n",
    "        # enforce value rules:\n",
    "        # - BG & INS must have numeric values\n",
    "        needs_value = df[\"code\"].isin(list(BG_CODES | INS_CODES))\n",
    "        df = df.loc[~needs_value | df[\"value\"].notna()].copy()\n",
    "\n",
    "        rows.append(df[[\"id\", \"datetime\", \"date\", \"time\", \"time_minutes\", \"code\", \"value\"]])\n",
    "\n",
    "    if not rows:\n",
    "        raise FileNotFoundError(f\"No files matched {pattern} under {data_dir}\")\n",
    "\n",
    "    out = pd.concat(rows, ignore_index=True)\n",
    "    out = out.sort_values([\"id\", \"datetime\", \"code\"], ascending=True).reset_index(drop=True)\n",
    "    out[\"code\"] = out[\"code\"].astype(int)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# Build supervised samples: next BG prediction\n",
    "# --------------------------------------------\n",
    "def build_next_glucose_supervised(\n",
    "    long_df: pd.DataFrame,\n",
    "    lookback_hours: int = 4,\n",
    "    require_next_within_hours: Optional[int] = None,  # e.g. 12 to avoid huge gaps\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    One row per glucose measurement event time t, predicting next glucose value at t_next.\n",
    "\n",
    "    Features are computed from events in (t - lookback, t], strictly causal.\n",
    "    \"\"\"\n",
    "    df = long_df.sort_values([\"id\", \"datetime\", \"code\"], ascending=True).copy()\n",
    "\n",
    "    samples: List[Dict] = []\n",
    "    lookback = timedelta(hours=lookback_hours)\n",
    "\n",
    "    for pid, sdf in df.groupby(\"id\", sort=False):\n",
    "        sdf = sdf.sort_values([\"datetime\", \"code\"]).reset_index(drop=True)\n",
    "\n",
    "        bg_events = sdf.loc[sdf[\"code\"].isin(BG_CODES), [\"datetime\", \"value\", \"time\", \"time_minutes\"]].copy()\n",
    "        bg_events = bg_events.loc[bg_events[\"value\"].notna()].reset_index(drop=True)\n",
    "\n",
    "        if bg_events.shape[0] < 2:\n",
    "            continue\n",
    "\n",
    "        # iterate each BG measurement, use next BG as target\n",
    "        for i in range(bg_events.shape[0] - 1):\n",
    "            t = bg_events.loc[i, \"datetime\"]\n",
    "            y_time = bg_events.loc[i + 1, \"datetime\"]\n",
    "            y_next = float(bg_events.loc[i + 1, \"value\"])\n",
    "\n",
    "            delta_min = int((y_time - t).total_seconds() // 60)\n",
    "            if require_next_within_hours is not None:\n",
    "                if delta_min > require_next_within_hours * 60:\n",
    "                    continue\n",
    "\n",
    "            w_start = t - lookback\n",
    "            hist = sdf.loc[(sdf[\"datetime\"] > w_start) & (sdf[\"datetime\"] <= t)].copy()\n",
    "            if hist.empty:\n",
    "                continue\n",
    "\n",
    "            # -------- Glucose history features (within window) --------\n",
    "            bg_hist = hist.loc[hist[\"code\"].isin(BG_CODES) & hist[\"value\"].notna(), [\"datetime\", \"value\"]].copy()\n",
    "            bg_hist = bg_hist.sort_values(\"datetime\")\n",
    "            bg_vals = bg_hist[\"value\"]\n",
    "\n",
    "            bg_stats = _series_stats(bg_vals)\n",
    "            bg_last = float(bg_vals.iloc[-1]) if bg_vals.shape[0] else np.nan\n",
    "            time_since_last_bg = (\n",
    "                int((t - bg_hist[\"datetime\"].iloc[-1]).total_seconds() // 60) if bg_hist.shape[0] else np.nan\n",
    "            )\n",
    "\n",
    "            # -------- Insulin features (within window) --------\n",
    "            ins_hist = hist.loc[hist[\"code\"].isin(INS_CODES) & hist[\"value\"].notna(), [\"datetime\", \"code\", \"value\"]].copy()\n",
    "            ins_hist = ins_hist.sort_values(\"datetime\")\n",
    "            ins_vals = ins_hist[\"value\"]\n",
    "\n",
    "            ins_stats = _series_stats(ins_vals)\n",
    "            time_since_last_ins = (\n",
    "                int((t - ins_hist[\"datetime\"].iloc[-1]).total_seconds() // 60) if ins_hist.shape[0] else np.nan\n",
    "            )\n",
    "\n",
    "            # per-insulin-code sums (useful, cheap)\n",
    "            ins_sum_by_code = (\n",
    "                ins_hist.groupby(\"code\")[\"value\"].sum().to_dict() if not ins_hist.empty else {}\n",
    "            )\n",
    "\n",
    "            # -------- Event flags / counts (within window) --------\n",
    "            codes = hist[\"code\"]\n",
    "            meal_count = int(codes.isin(MEAL_CODES).sum())\n",
    "            exercise_count = int(codes.isin(EXERCISE_CODES).sum())\n",
    "            hypo_flag = bool((codes == HYPO_CODE).any())\n",
    "            special_flag = bool((codes == SPECIAL_CODE).any())\n",
    "\n",
    "            # paper-like time proxy for the anchor time t\n",
    "            is_paper_like_time_flag = bool(bg_events.loc[i, \"time\"] in PAPER_TIMES)\n",
    "\n",
    "            # -------- Temporal context --------\n",
    "            hour_of_day = int(t.hour)\n",
    "            day_of_week = int(t.dayofweek)\n",
    "            is_weekend = bool(day_of_week in (5, 6))\n",
    "\n",
    "            row = {\n",
    "                \"id\": pid,\n",
    "                \"event_time\": t,\n",
    "                \"target_time\": y_time,\n",
    "                \"delta_t_minutes\": delta_min,\n",
    "                \"y_next_glucose\": y_next,\n",
    "\n",
    "                # temporal context\n",
    "                \"hour_of_day\": hour_of_day,\n",
    "                \"day_of_week\": day_of_week,\n",
    "                \"is_weekend\": is_weekend,\n",
    "                \"is_paper_like_time_flag\": is_paper_like_time_flag,\n",
    "\n",
    "                # glucose window stats\n",
    "                \"bg_last_value\": bg_last,\n",
    "                \"bg_window_count\": int(bg_stats[\"count\"]),\n",
    "                \"bg_window_mean\": bg_stats[\"mean\"],\n",
    "                \"bg_window_std\": bg_stats[\"std\"],\n",
    "                \"bg_window_min\": bg_stats[\"min\"],\n",
    "                \"bg_window_max\": bg_stats[\"max\"],\n",
    "                \"bg_window_range\": bg_stats[\"range\"],\n",
    "                \"time_since_last_bg\": time_since_last_bg,\n",
    "\n",
    "                # insulin window stats\n",
    "                \"ins_window_count\": int(ins_stats[\"count\"]),\n",
    "                \"ins_window_sum\": ins_stats[\"sum\"],\n",
    "                \"ins_window_mean\": ins_stats[\"mean\"],\n",
    "                \"ins_window_std\": ins_stats[\"std\"],\n",
    "                \"ins_window_max\": ins_stats[\"max\"],\n",
    "                \"time_since_last_ins\": time_since_last_ins,\n",
    "\n",
    "                # insulin per code sums\n",
    "                \"ins_33_sum\": float(ins_sum_by_code.get(33, 0.0)),\n",
    "                \"ins_34_sum\": float(ins_sum_by_code.get(34, 0.0)),\n",
    "                \"ins_35_sum\": float(ins_sum_by_code.get(35, 0.0)),\n",
    "\n",
    "                # event features\n",
    "                \"meal_events_count\": meal_count,\n",
    "                \"exercise_events_count\": exercise_count,\n",
    "                \"hypo_event_flag\": hypo_flag,\n",
    "                \"special_event_flag\": special_flag,\n",
    "            }\n",
    "\n",
    "            samples.append(row)\n",
    "\n",
    "    sup = pd.DataFrame(samples)\n",
    "    if sup.empty:\n",
    "        return sup\n",
    "\n",
    "    # enforce dtypes & deterministic filling\n",
    "    sup = sup.sort_values([\"id\", \"event_time\"]).reset_index(drop=True)\n",
    "    return sup\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# Run end-to-end in this cell\n",
    "# --------------------------------------------\n",
    "bad_files = [\"data-02\", \"data-27\", \"data-29\", \"data-40\", \"data-67\"]  # reuse your known-bad list\n",
    "\n",
    "long_df = read_diabetes_files_long(DATA_DIR / \"raw\", pattern=\"data-*\", bad_files=bad_files)\n",
    "\n",
    "# supervised samples: next glucose prediction\n",
    "supervised_df = build_next_glucose_supervised(\n",
    "    long_df,\n",
    "    lookback_hours=4,                # change to 2/6 as experiments\n",
    "    require_next_within_hours=12,    # optional: filters very large gaps; set None to disable\n",
    ")\n",
    "\n",
    "OUT_DIR = DATA_DIR / \"processed\" / \"supervised\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "supervised_df.to_parquet(\n",
    "    OUT_DIR / \"diabetes_next_glucose_supervised.parquet\",\n",
    "    engine=\"fastparquet\",\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "print(supervised_df.shape)\n",
    "print(supervised_df.head())\n",
    "print(\"Unique patients:\", supervised_df[\"id\"].nunique())"
   ],
   "id": "78eaa804f5f85562",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10612, 30)\n",
      "        id          event_time         target_time  delta_t_minutes  \\\n",
      "0  data-01 1991-04-21 09:09:00 1991-04-21 17:08:00              479   \n",
      "1  data-01 1991-04-21 17:08:00 1991-04-21 22:51:00              343   \n",
      "2  data-01 1991-04-21 22:51:00 1991-04-22 07:35:00              524   \n",
      "3  data-01 1991-04-22 07:35:00 1991-04-22 16:56:00              561   \n",
      "4  data-01 1991-04-23 07:25:00 1991-04-23 17:25:00              600   \n",
      "\n",
      "   y_next_glucose  hour_of_day  day_of_week  is_weekend  \\\n",
      "0           119.0            9            6        True   \n",
      "1           123.0           17            6        True   \n",
      "2           216.0           22            6        True   \n",
      "3           211.0            7            0       False   \n",
      "4           129.0            7            1       False   \n",
      "\n",
      "   is_paper_like_time_flag  bg_last_value  ...  ins_window_std  \\\n",
      "0                    False          100.0  ...        2.828427   \n",
      "1                    False          119.0  ...        0.000000   \n",
      "2                    False          123.0  ...             NaN   \n",
      "3                    False          216.0  ...        2.121320   \n",
      "4                    False          257.0  ...        1.414214   \n",
      "\n",
      "   ins_window_max  time_since_last_ins  ins_33_sum  ins_34_sum  ins_35_sum  \\\n",
      "0            13.0                  0.0         9.0        13.0         0.0   \n",
      "1             7.0                  0.0         7.0         0.0         0.0   \n",
      "2             NaN                  NaN         0.0         0.0         0.0   \n",
      "3            13.0                  0.0        10.0        13.0         0.0   \n",
      "4            13.0                  0.0        11.0        13.0         0.0   \n",
      "\n",
      "   meal_events_count  exercise_events_count  hypo_event_flag  \\\n",
      "0                  0                      0            False   \n",
      "1                  0                      0            False   \n",
      "2                  0                      0            False   \n",
      "3                  0                      0            False   \n",
      "4                  0                      0            False   \n",
      "\n",
      "   special_event_flag  \n",
      "0               False  \n",
      "1               False  \n",
      "2               False  \n",
      "3               False  \n",
      "4               False  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "Unique patients: 65\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a11f07f572b0cf2a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
