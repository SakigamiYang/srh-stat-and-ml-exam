{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "# Transform Raw Data to Sample Data",
   "id": "a180f7fd638534b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T15:21:48.201898Z",
     "start_time": "2026-02-07T15:21:47.969698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!uv pip install \\\n",
    "  \"fastparquet>=2025.12.0\" \\\n",
    "  \"ipykernel>=7.2.0\" \\\n",
    "  \"llvmlite>=0.42\" \\\n",
    "  \"matplotlib>=3.10.8\" \\\n",
    "  \"numba>=0.59\" \\\n",
    "  \"numpy>=2.3.5\" \\\n",
    "  \"pandas==2.3.3\" \\\n",
    "  \"pandas-stubs==2.3.3.260113\" \\\n",
    "  \"prophet>=1.3.0\" \\\n",
    "  \"scikit-learn>=1.8.0\" \\\n",
    "  \"scikit-learn>=1.8.0\" \\\n",
    "  \"scipy>=1.17.0\" \\\n",
    "  \"seaborn>=0.13.2\" \\\n",
    "  \"umap-learn>=0.5.11\""
   ],
   "id": "4d40e02b01deb921",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2mUsing Python 3.12.12 environment at: /Users/z.yang/playground/srh-stat-and-ml-exam/.venv\u001B[0m\r\n",
      "\u001B[2mAudited \u001B[1m14 packages\u001B[0m \u001B[2min 31ms\u001B[0m\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Because supervised and unsupervised learning require different data formats, the same dataset is transformed into two separate sample datasets according to the problem definition.",
   "id": "b7a0cf807699571c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T15:21:48.651875Z",
     "start_time": "2026-02-07T15:21:48.622418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT_DIR = Path().resolve().parent"
   ],
   "id": "8b24eb0ef510649a",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T15:21:49.007628Z",
     "start_time": "2026-02-07T15:21:48.993310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"future.no_silent_downcasting\", True)"
   ],
   "id": "bc39f7e0a301f6a5",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Format Check",
   "id": "9e90b7a1e3439f1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T15:21:50.689056Z",
     "start_time": "2026-02-07T15:21:50.630795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "DATE_RE = re.compile(r\"^\\d{2}-\\d{2}-\\d{4}$\")                 # MM-DD-YYYY\n",
    "TIME_RE = re.compile(r\"^\\d{1,2}:\\d{2}$\")                     # H:MM or HH:MM\n",
    "INT_RE  = re.compile(r\"^-?\\d+$\")                             # integer\n",
    "NUM_RE  = re.compile(r\"^-?\\d+(\\.\\d+)?$\")                     # number (int/float)\n",
    "\n",
    "\n",
    "def is_valid_record_line(line: str) -> bool:\n",
    "    \"\"\"\n",
    "    Accepts a raw line. Returns True if it matches the expected 4-field format:\n",
    "    date<TAB>time<TAB>code<TAB>value\n",
    "    \"\"\"\n",
    "    line = line.rstrip(\"\\n\")\n",
    "    if not line.strip():\n",
    "        return True  # ignore empty/whitespace-only lines\n",
    "\n",
    "    parts = line.split(\"\\t\")\n",
    "    if len(parts) != 4:\n",
    "        return False\n",
    "\n",
    "    date_s, time_s, code_s, value_s = [p.strip() for p in parts]\n",
    "\n",
    "    # must be non-empty\n",
    "    if not date_s or not time_s or not code_s or not value_s:\n",
    "        return False\n",
    "\n",
    "    if not DATE_RE.match(date_s):\n",
    "        return False\n",
    "    if not TIME_RE.match(time_s):\n",
    "        return False\n",
    "    if not INT_RE.match(code_s):\n",
    "        return False\n",
    "    if not NUM_RE.match(value_s):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def scan_file(fp: Path, max_examples: int = 5) -> Tuple[bool, List[Tuple[int, str]]]:\n",
    "    \"\"\"\n",
    "    Returns (is_bad, examples). examples are (line_no, raw_line) for invalid lines.\n",
    "    \"\"\"\n",
    "    examples: List[Tuple[int, str]] = []\n",
    "    with fp.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        for i, line in enumerate(f, start=1):\n",
    "            if not is_valid_record_line(line):\n",
    "                if len(examples) < max_examples:\n",
    "                    examples.append((i, line.rstrip(\"\\n\")))\n",
    "                # keep scanning to decide bad file; but examples limited\n",
    "    return (len(examples) > 0), examples\n",
    "\n",
    "\n",
    "def find_bad_files(data_dir: Path, pattern: str = \"data-*\", max_examples: int = 5) -> None:\n",
    "    bad_files: List[Path] = []\n",
    "    details = {}\n",
    "\n",
    "    for fp in sorted(data_dir.glob(pattern)):\n",
    "        if fp.is_dir():\n",
    "            continue\n",
    "        is_bad, examples = scan_file(fp, max_examples=max_examples)\n",
    "        if is_bad:\n",
    "            bad_files.append(fp)\n",
    "            details[fp.name] = examples\n",
    "\n",
    "    # 1) output bad file list\n",
    "    out_list = data_dir / \"bad_format_files.txt\"\n",
    "    with out_list.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for fp in bad_files:\n",
    "            f.write(fp.name + \"\\n\")\n",
    "\n",
    "    # 2) output examples per bad file\n",
    "    out_examples = data_dir / \"bad_format_examples.txt\"\n",
    "    with out_examples.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for name in sorted(details.keys()):\n",
    "            f.write(f\"== {name} ==\\n\")\n",
    "            for line_no, raw in details[name]:\n",
    "                f.write(f\"  line {line_no}: {raw}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    print(f\"Scanned dir: {data_dir.resolve()}\")\n",
    "    print(f\"Bad files: {len(bad_files)}\")\n",
    "    print(f\"Bad file names: {[fp.name for fp in bad_files]}\")\n",
    "    print(f\"Written: {out_list}  (file names only)\")\n",
    "    print(f\"Written: {out_examples}  (first {max_examples} invalid lines per file)\")\n",
    "\n",
    "\n",
    "# change this to your folder that contains data-01..data-70\n",
    "DATA_DIR = ROOT_DIR / \"data\" / \"raw\"\n",
    "find_bad_files(DATA_DIR, pattern=\"data-*\", max_examples=5)"
   ],
   "id": "82e009ba7fe53b07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanned dir: /Users/z.yang/playground/srh-stat-and-ml-exam/data/raw\n",
      "Bad files: 5\n",
      "Bad file names: ['data-02', 'data-27', 'data-29', 'data-40', 'data-67']\n",
      "Written: /Users/z.yang/playground/srh-stat-and-ml-exam/data/raw/bad_format_files.txt  (file names only)\n",
      "Written: /Users/z.yang/playground/srh-stat-and-ml-exam/data/raw/bad_format_examples.txt  (first 5 invalid lines per file)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Unsupervised Learning",
   "id": "f76502c6f166917e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T15:21:56.404590Z",
     "start_time": "2026-02-07T15:21:53.424396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "BG_CODES = [48, 57, 58, 59, 60, 61, 62, 63, 64]\n",
    "INS_CODES = [33, 34, 35]\n",
    "EVT_CODES = [65, 66, 67, 68, 69, 70, 71, 72]\n",
    "PAPER_TIMES = {\"08:00\", \"12:00\", \"18:00\", \"22:00\"}\n",
    "\n",
    "\n",
    "def _to_minutes(hhmm: str) -> int:\n",
    "    h, m = hhmm.split(\":\")\n",
    "    return int(h) * 60 + int(m)\n",
    "\n",
    "\n",
    "def _quantile(x: pd.Series, q: float) -> float:\n",
    "    # robust quantile helper (returns nan if empty)\n",
    "    if x.empty:\n",
    "        return np.nan\n",
    "    return float(x.quantile(q))\n",
    "\n",
    "\n",
    "def _stats_for_series(x: pd.Series) -> dict:\n",
    "    # x is numeric series already\n",
    "    if x.empty:\n",
    "        return {\n",
    "            \"count\": 0,\n",
    "            \"min\": np.nan,\n",
    "            \"max\": np.nan,\n",
    "            \"mean\": np.nan,\n",
    "            \"median\": np.nan,\n",
    "            \"std\": np.nan,\n",
    "            \"first\": np.nan,\n",
    "            \"last\": np.nan,\n",
    "            \"range\": np.nan,\n",
    "        }\n",
    "    x_sorted = x  # assume caller has sorted by time if needed\n",
    "    mn = float(x_sorted.min())\n",
    "    mx = float(x_sorted.max())\n",
    "    return {\n",
    "        \"count\": int(x_sorted.shape[0]),\n",
    "        \"min\": mn,\n",
    "        \"max\": mx,\n",
    "        \"mean\": float(x_sorted.mean()),\n",
    "        \"median\": float(x_sorted.median()),\n",
    "        \"std\": float(x_sorted.std(ddof=1)) if x_sorted.shape[0] > 1 else 0.0,\n",
    "        \"first\": float(x_sorted.iloc[0]),\n",
    "        \"last\": float(x_sorted.iloc[-1]),\n",
    "        \"range\": float(mx - mn),\n",
    "    }\n",
    "\n",
    "\n",
    "def read_diabetes_files(\n",
    "        data_dir: Path,\n",
    "        pattern: str = \"data-*\",\n",
    "        bad_files: List[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads all diabetes files into one long table:\n",
    "    columns: id, date, time, code, value\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    bad_files = bad_files or []\n",
    "    for fp in sorted(data_dir.glob(pattern)):\n",
    "        if fp.is_dir():\n",
    "            continue\n",
    "        file_id = fp.name  # expects \"data-01\", ...\n",
    "        if file_id in bad_files:  # Skip files with bad format\n",
    "            continue\n",
    "        df = pd.read_csv(\n",
    "            fp,\n",
    "            sep=\"\\t\",\n",
    "            header=None,\n",
    "            names=[\"date_str\", \"time_str\", \"code\", \"value_str\"],\n",
    "            dtype={\"date_str\": \"string\", \"time_str\": \"string\", \"code\": \"int64\", \"value_str\": \"string\"},\n",
    "        )\n",
    "        df[\"id\"] = file_id\n",
    "        rows.append(df)\n",
    "\n",
    "    if not rows:\n",
    "        raise FileNotFoundError(f\"No files matched {pattern} under {data_dir}\")\n",
    "\n",
    "    out = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "    # parse date/time\n",
    "    out[\"date\"] = pd.to_datetime(out[\"date_str\"], format=\"%m-%d-%Y\", errors=\"coerce\")\n",
    "    out[\"time\"] = out[\"time_str\"].astype(str).str.zfill(5)  # e.g. \"9:09\" -> \"09:09\" if needed\n",
    "    out[\"time_minutes\"] = out[\"time\"].map(_to_minutes)\n",
    "\n",
    "    # parse value: keep numeric, tolerate leading zeros like \"009\"\n",
    "    out[\"value\"] = pd.to_numeric(out[\"value_str\"], errors=\"coerce\")\n",
    "\n",
    "    # basic validation\n",
    "    bad = out[\"date\"].isna() | out[\"time_minutes\"].isna() | out[\"value\"].isna()\n",
    "    if bad.any():\n",
    "        # keep them out, but you can also raise\n",
    "        out = out.loc[~bad].copy()\n",
    "\n",
    "    return out[[\"id\", \"date\", \"time\", \"time_minutes\", \"code\", \"value\"]]\n",
    "\n",
    "\n",
    "def build_daily_features(long_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Input: long_df with columns: id, date, time, time_minutes, code, value\n",
    "    Output: wide daily feature table\n",
    "    \"\"\"\n",
    "    # sort for first/last features\n",
    "    long_df = long_df.sort_values([\"id\", \"date\", \"time_minutes\", \"code\"], ascending=True).copy()\n",
    "\n",
    "    # base daily aggregates (time rhythm + counts)\n",
    "    g = long_df.groupby([\"id\", \"date\"], sort=False)\n",
    "\n",
    "    base = g.agg(\n",
    "        n_events=(\"code\", \"size\"),\n",
    "        n_unique_timestamps=(\"time\", \"nunique\"),\n",
    "        first_time_minutes=(\"time_minutes\", \"min\"),\n",
    "        last_time_minutes=(\"time_minutes\", \"max\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    base[\"active_span_minutes\"] = base[\"last_time_minutes\"] - base[\"first_time_minutes\"]\n",
    "\n",
    "    # time buckets (record counts)\n",
    "    def _bucket_counts(sub: pd.DataFrame) -> pd.Series:\n",
    "        t = sub[\"time_minutes\"].to_numpy()\n",
    "        # [0, 300) night (00:00-04:59)\n",
    "        night = int(((t >= 0) & (t < 300)).sum())\n",
    "        morning = int(((t >= 300) & (t < 720)).sum())      # 05:00-11:59\n",
    "        afternoon = int(((t >= 720) & (t < 1080)).sum())   # 12:00-17:59\n",
    "        evening = int(((t >= 1080) & (t < 1440)).sum())    # 18:00-23:59\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"events_night_count\": night,\n",
    "                \"events_morning_count\": morning,\n",
    "                \"events_afternoon_count\": afternoon,\n",
    "                \"events_evening_count\": evening,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    buckets = g.apply(_bucket_counts, include_groups=False).reset_index()\n",
    "\n",
    "    # paper-like time flag: share of events at fixed logical times\n",
    "    def _paper_like_flag(sub: pd.DataFrame) -> bool:\n",
    "        # normalize \"09:09\" etc already\n",
    "        share = (sub[\"time\"].isin(PAPER_TIMES)).mean()\n",
    "        return bool(share >= 0.6)  # threshold; adjust as needed\n",
    "\n",
    "    paper_flag = g.apply(\n",
    "        _paper_like_flag,\n",
    "        include_groups=False).reset_index(name=\"has_paper_like_times_flag\")\n",
    "\n",
    "    # add calendar features\n",
    "    base[\"dow\"] = pd.to_datetime(base[\"date\"]).dt.weekday.astype(int)\n",
    "    base[\"is_weekend\"] = base[\"dow\"].isin([5, 6])\n",
    "\n",
    "    # per-code stats builder\n",
    "    def build_code_stats(codes: Iterable[int], prefix: str) -> pd.DataFrame:\n",
    "        pieces = []\n",
    "        for c in codes:\n",
    "            sub = long_df.loc[long_df[\"code\"] == c, [\"id\", \"date\", \"time_minutes\", \"value\"]].copy()\n",
    "            if sub.empty:\n",
    "                # create empty frame with expected columns later via merge\n",
    "                continue\n",
    "            sub = sub.sort_values([\"id\", \"date\", \"time_minutes\"])\n",
    "            gs = sub.groupby([\"id\", \"date\"], sort=False)[\"value\"].apply(\n",
    "                lambda s: pd.Series(_stats_for_series(s)))\n",
    "            gs = gs.unstack()  # columns are the stats keys\n",
    "            gs = gs.add_prefix(f\"{prefix}_{c}_\").reset_index()\n",
    "            # count should be int, keep others float\n",
    "            if f\"{prefix}_{c}_count\" in gs.columns:\n",
    "                gs[f\"{prefix}_{c}_count\"] = gs[f\"{prefix}_{c}_count\"].fillna(0).astype(int)\n",
    "            pieces.append(gs)\n",
    "\n",
    "        if not pieces:\n",
    "            return base[[\"id\", \"date\"]].copy()\n",
    "        out = pieces[0]\n",
    "        for p in pieces[1:]:\n",
    "            out = out.merge(p, on=[\"id\", \"date\"], how=\"outer\")\n",
    "        return out\n",
    "\n",
    "    bg_stats = build_code_stats(BG_CODES, \"bg\")\n",
    "    ins_stats = build_code_stats(INS_CODES, \"ins\")\n",
    "\n",
    "    # merged BG \"all\" stats\n",
    "    bg_all = long_df.loc[long_df[\"code\"].isin(BG_CODES), [\"id\", \"date\", \"time_minutes\", \"value\"]].copy()\n",
    "    if not bg_all.empty:\n",
    "        bg_all = bg_all.sort_values([\"id\", \"date\", \"time_minutes\"])\n",
    "        bg_all_g = bg_all.groupby([\"id\", \"date\"], sort=False)[\"value\"].apply(\n",
    "            lambda s: pd.Series(_stats_for_series(s)))\n",
    "        bg_all_g = bg_all_g.unstack().add_prefix(\"bg_all_\").reset_index()\n",
    "        bg_all_g[\"bg_all_count\"] = bg_all_g[\"bg_all_count\"].fillna(0).astype(int)\n",
    "    else:\n",
    "        bg_all_g = base[[\"id\", \"date\"]].copy()\n",
    "\n",
    "    # merged INS \"all\" stats\n",
    "    ins_all = long_df.loc[long_df[\"code\"].isin(INS_CODES), [\"id\", \"date\", \"time_minutes\", \"value\"]].copy()\n",
    "    if not ins_all.empty:\n",
    "        ins_all = ins_all.sort_values([\"id\", \"date\", \"time_minutes\"])\n",
    "        ins_all_g = ins_all.groupby([\"id\", \"date\"], sort=False)[\"value\"].apply(\n",
    "            lambda s: pd.Series(_stats_for_series(s)))\n",
    "        ins_all_g = ins_all_g.unstack().add_prefix(\"ins_all_\").reset_index()\n",
    "        ins_all_g[\"ins_all_count\"] = ins_all_g[\"ins_all_count\"].fillna(0).astype(int)\n",
    "        # \"sum\" for insulin all: add explicitly\n",
    "        ins_sum = ins_all.groupby([\"id\", \"date\"], sort=False)[\"value\"].sum().reset_index(name=\"ins_all_sum\")\n",
    "        ins_all_g = ins_all_g.merge(ins_sum, on=[\"id\", \"date\"], how=\"left\")\n",
    "    else:\n",
    "        ins_all_g = base[[\"id\", \"date\"]].copy()\n",
    "\n",
    "    # per-code insulin sum (dose sum is useful)\n",
    "    for c in INS_CODES:\n",
    "        col = f\"ins_{c}_sum\"\n",
    "        if col not in ins_stats.columns:\n",
    "            # compute sum per code\n",
    "            tmp = long_df.loc[long_df[\"code\"] == c].groupby([\"id\", \"date\"], sort=False)[\"value\"].sum().reset_index(name=col)\n",
    "            ins_stats = ins_stats.merge(tmp, on=[\"id\", \"date\"], how=\"left\")\n",
    "\n",
    "    # events: count + flag\n",
    "    evt = long_df.loc[long_df[\"code\"].isin(EVT_CODES), [\"id\", \"date\", \"code\"]].copy()\n",
    "    if evt.empty:\n",
    "        evt_wide = base[[\"id\", \"date\"]].copy()\n",
    "    else:\n",
    "        evt_counts = (\n",
    "            evt.groupby([\"id\", \"date\", \"code\"], sort=False)\n",
    "            .size()\n",
    "            .rename(\"count\")\n",
    "            .reset_index()\n",
    "        )\n",
    "        # pivot counts\n",
    "        evt_wide = evt_counts.pivot_table(index=[\"id\", \"date\"], columns=\"code\", values=\"count\", fill_value=0, aggfunc=\"sum\")\n",
    "        evt_wide.columns = [f\"evt_{int(c)}_count\" for c in evt_wide.columns]\n",
    "        evt_wide = evt_wide.reset_index()\n",
    "        # flags\n",
    "        for c in EVT_CODES:\n",
    "            ccol = f\"evt_{c}_count\"\n",
    "            fcol = f\"evt_{c}_flag\"\n",
    "            if ccol not in evt_wide.columns:\n",
    "                evt_wide[ccol] = 0\n",
    "            evt_wide[fcol] = evt_wide[ccol].astype(int) > 0\n",
    "\n",
    "    # rollups\n",
    "    def ensure_col(df: pd.DataFrame, col: str, default=0):\n",
    "        if col not in df.columns:\n",
    "            df[col] = default\n",
    "        return df\n",
    "\n",
    "    evt_wide = ensure_col(evt_wide, \"evt_66_count\", 0)\n",
    "    evt_wide = ensure_col(evt_wide, \"evt_67_count\", 0)\n",
    "    evt_wide = ensure_col(evt_wide, \"evt_68_count\", 0)\n",
    "    evt_wide = ensure_col(evt_wide, \"evt_69_count\", 0)\n",
    "    evt_wide = ensure_col(evt_wide, \"evt_70_count\", 0)\n",
    "    evt_wide = ensure_col(evt_wide, \"evt_71_count\", 0)\n",
    "    evt_wide = ensure_col(evt_wide, \"evt_65_flag\", False)\n",
    "    evt_wide = ensure_col(evt_wide, \"evt_72_flag\", False)\n",
    "\n",
    "    evt_wide[\"meal_events_count\"] = evt_wide[\"evt_66_count\"] + evt_wide[\"evt_67_count\"] + evt_wide[\"evt_68_count\"]\n",
    "    evt_wide[\"meal_more_flag\"] = ensure_col(evt_wide, \"evt_67_flag\", False)[\"evt_67_flag\"]\n",
    "    evt_wide[\"meal_less_flag\"] = ensure_col(evt_wide, \"evt_68_flag\", False)[\"evt_68_flag\"]\n",
    "\n",
    "    evt_wide[\"exercise_events_count\"] = evt_wide[\"evt_69_count\"] + evt_wide[\"evt_70_count\"] + evt_wide[\"evt_71_count\"]\n",
    "    evt_wide[\"exercise_more_flag\"] = ensure_col(evt_wide, \"evt_70_flag\", False)[\"evt_70_flag\"]\n",
    "    evt_wide[\"exercise_less_flag\"] = ensure_col(evt_wide, \"evt_71_flag\", False)[\"evt_71_flag\"]\n",
    "\n",
    "    evt_wide[\"hypo_flag\"] = ensure_col(evt_wide, \"evt_65_flag\", False)[\"evt_65_flag\"]\n",
    "    evt_wide[\"special_flag\"] = ensure_col(evt_wide, \"evt_72_flag\", False)[\"evt_72_flag\"]\n",
    "\n",
    "    # missing flags\n",
    "    bg_presence = long_df.groupby([\"id\", \"date\"], sort=False)[\"code\"].apply(lambda s: bool(s.isin(BG_CODES).any())).reset_index(name=\"has_bg\")\n",
    "    ins_presence = long_df.groupby([\"id\", \"date\"], sort=False)[\"code\"].apply(lambda s: bool(s.isin(INS_CODES).any())).reset_index(name=\"has_ins\")\n",
    "\n",
    "    # merge everything\n",
    "    daily = (\n",
    "        base.merge(buckets, on=[\"id\", \"date\"], how=\"left\")\n",
    "            .merge(paper_flag, on=[\"id\", \"date\"], how=\"left\")\n",
    "            .merge(bg_stats, on=[\"id\", \"date\"], how=\"left\")\n",
    "            .merge(bg_all_g, on=[\"id\", \"date\"], how=\"left\")\n",
    "            .merge(ins_stats, on=[\"id\", \"date\"], how=\"left\")\n",
    "            .merge(ins_all_g, on=[\"id\", \"date\"], how=\"left\")\n",
    "            .merge(evt_wide, on=[\"id\", \"date\"], how=\"left\")\n",
    "            .merge(bg_presence, on=[\"id\", \"date\"], how=\"left\")\n",
    "            .merge(ins_presence, on=[\"id\", \"date\"], how=\"left\")\n",
    "    )\n",
    "\n",
    "    # finalize flags\n",
    "    daily[\"has_paper_like_times_flag\"] = daily[\"has_paper_like_times_flag\"].fillna(False).astype(bool)\n",
    "    daily[\"missing_bg_flag\"] = (~daily[\"has_bg\"].fillna(False)).astype(bool)\n",
    "    daily[\"missing_insulin_flag\"] = (~daily[\"has_ins\"].fillna(False)).astype(bool)\n",
    "    daily = daily.drop(columns=[\"has_bg\", \"has_ins\"])\n",
    "\n",
    "    # fill count columns that may be missing due to merges\n",
    "    for c in BG_CODES:\n",
    "        cnt = f\"bg_{c}_count\"\n",
    "        if cnt in daily.columns:\n",
    "            daily[cnt] = daily[cnt].fillna(0).astype(int)\n",
    "    for c in INS_CODES:\n",
    "        cnt = f\"ins_{c}_count\"\n",
    "        if cnt in daily.columns:\n",
    "            daily[cnt] = daily[cnt].fillna(0).astype(int)\n",
    "    for c in EVT_CODES:\n",
    "        cnt = f\"evt_{c}_count\"\n",
    "        flg = f\"evt_{c}_flag\"\n",
    "        if cnt in daily.columns:\n",
    "            daily[cnt] = daily[cnt].fillna(0).astype(int)\n",
    "        if flg in daily.columns:\n",
    "            daily[flg] = daily[flg].astype(\"boolean\").fillna(False).astype(bool)\n",
    "\n",
    "    # ensure deterministic column order: keys first\n",
    "    key_cols = [\"id\", \"date\"]\n",
    "    other_cols = [c for c in daily.columns if c not in key_cols]\n",
    "    daily = daily[key_cols + other_cols].sort_values([\"id\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "    return daily\n",
    "\n",
    "\n",
    "def fill_missing_values(daily_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = daily_df.copy()\n",
    "\n",
    "    for c in df.columns:\n",
    "        if c.endswith(\"_flag\"):\n",
    "            df[c] = (\n",
    "                df[c]\n",
    "                .astype(\"boolean\")\n",
    "                .fillna(False)\n",
    "                .astype(bool)\n",
    "            )\n",
    "\n",
    "        elif c.endswith(\"_count\"):\n",
    "            df[c] = (\n",
    "                df[c]\n",
    "                .fillna(0)\n",
    "                .astype(int)\n",
    "            )\n",
    "\n",
    "        elif c not in [\"id\", \"date\"]:\n",
    "            df[c] = df[c].fillna(0)\n",
    "\n",
    "    assert df.isna().sum().sum() == 0\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "DATA_DIR = ROOT_DIR / \"data\" / \"raw\"\n",
    "bad_files = [\"data-02\", \"data-27\", \"data-29\", \"data-40\", \"data-67\"]\n",
    "long_df = read_diabetes_files(DATA_DIR, pattern=\"data-*\", bad_files=bad_files)\n",
    "daily_df = build_daily_features(long_df)\n",
    "daily_df = fill_missing_values(daily_df)\n",
    "\n",
    "OUT_DIR = ROOT_DIR / \"data\" / \"processed\" / \"unsupervised\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "daily_df.to_parquet(OUT_DIR / \"diabetes_daily_features.parquet\", engine=\"fastparquet\", index=False)\n",
    "print(daily_df.shape)\n",
    "print(daily_df.head())"
   ],
   "id": "f9aacc8cd49f9ea6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3378, 170)\n",
      "        id       date  n_events  n_unique_timestamps  first_time_minutes  \\\n",
      "0  data-01 1991-04-21         6                    3                 549   \n",
      "1  data-01 1991-04-22         6                    3                 455   \n",
      "2  data-01 1991-04-23         5                    2                 445   \n",
      "3  data-01 1991-04-24         7                    4                 472   \n",
      "4  data-01 1991-04-25         8                    4                 449   \n",
      "\n",
      "   last_time_minutes  active_span_minutes  dow  is_weekend  \\\n",
      "0               1371                  822    6        True   \n",
      "1               1016                  561    0       False   \n",
      "2               1045                  600    1       False   \n",
      "3               1329                  857    2       False   \n",
      "4               1314                  865    3       False   \n",
      "\n",
      "   events_night_count  ...  meal_events_count  meal_more_flag  meal_less_flag  \\\n",
      "0                   0  ...                  0           False           False   \n",
      "1                   0  ...                  0           False           False   \n",
      "2                   0  ...                  0           False           False   \n",
      "3                   0  ...                  0           False           False   \n",
      "4                   0  ...                  0           False           False   \n",
      "\n",
      "   exercise_events_count  exercise_more_flag  exercise_less_flag  hypo_flag  \\\n",
      "0                      0               False               False      False   \n",
      "1                      0               False               False      False   \n",
      "2                      0               False               False      False   \n",
      "3                      0               False               False      False   \n",
      "4                      0               False               False      False   \n",
      "\n",
      "   special_flag  missing_bg_flag  missing_insulin_flag  \n",
      "0         False            False                 False  \n",
      "1         False            False                 False  \n",
      "2         False            False                 False  \n",
      "3         False            False                 False  \n",
      "4         False            False                 False  \n",
      "\n",
      "[5 rows x 170 columns]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T15:20:56.833404Z",
     "start_time": "2026-02-07T15:20:56.828515Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "fd1106ec7bb4a9e3",
   "outputs": [],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
